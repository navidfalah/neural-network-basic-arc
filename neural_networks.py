# -*- coding: utf-8 -*-
"""neural_networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s5X_GHCABilgaGR7fxtSevUgnHksNrji
"""

### neural networks (deep learning)
!pip install mglearn
import mglearn

display(mglearn.plots.plot_logistic_regression_graph())

display(mglearn.plots.plot_single_hidden_layer_graph())

import numpy as np
import matplotlib.pyplot as plt

line = np.linspace(-3, 3, 100)
plt.plot(line, np.tanh(line), label="tanh")
plt.plot(line, np.maximum(line, 0), label="relu", c='r')
plt.legend(loc="best")
plt.title("activation functions")
plt.show()

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

X, y = make_moons(n_samples=100, noise=0.25, random_state=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)

plt.xlabel("feature 0")
plt.ylabel("feature 1")
plt.legend()
plt.show()

from sklearn.neural_network import MLPClassifier

mpl_reduced = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[4, 10])
mpl_reduced.fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mpl_reduced, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
plt.xlabel("feature 0")
plt.ylabel("feature 1")
plt.legend()
plt.show()

### using tan for the mlp

mpl_tan = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[5, 10], activation='tanh')
mpl_tan.fit(X_train, y_train)
mglearn.plots.plot_2d_separator(mpl_tan, X_train, fill=True, alpha=.3)
mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)
plt.xlabel("feature 0")
plt.ylabel("feature 1")
plt.legend()
plt.show()

fig, axes = plt.subplots(3, 5, figsize=(20, 8))

for axx, n_hidden_nodes in zip(axes, [1, 10, 100]):
  for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1, 10, 100]):
    mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[n_hidden_nodes, 10], alpha=alpha)
    mlp.fit(X_train, y_train)
    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)
    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)
    ax.set_title("n_hidden=[{}, 10]\nalpha={:.4f}".format(n_hidden_nodes, alpha))

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)

mlp = MLPClassifier(random_state=42)
mlp.fit(X_train, y_train)

print("Accuracy on training set: {:.2f}".format(mlp.score(X_train, y_train)))
print("Accuracy on test set: {:.2f}".format(mlp.score(X_test, y_test)))

### scale the data

mean_on_train = X_train.mean(axis=0)
std_on_train = X_train.std(axis=0)

X_train_scaled = (X_train - mean_on_train) / std_on_train
X_test_scaled = (X_test - mean_on_train) / std_on_train

mpl = MLPClassifier(random_state=0)
mpl.fit(X_train_scaled, y_train)

print("Accuracy on training set: {:.3f}".format(mpl.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(mpl.score(X_test_scaled, y_test)))

mpl_itr = MLPClassifier(max_iter=1000, random_state=0)
mpl_itr.fit(X_train_scaled, y_train)

print("Accuracy on training set: {:.3f}".format(mpl_itr.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(mpl_itr.score(X_test_scaled, y_test)))

mlp_generalize = MLPClassifier(max_iter=1000, alpha=1, random_state=0)
mlp_generalize.fit(X_train_scaled, y_train)

print("Accuracy on training set: {:.3f}".format(mlp_generalize.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(mlp_generalize.score(X_test_scaled, y_test)))

plt.figure(figsize=(20, 5))
plt.imshow(mlp_generalize.coefs_[0], interpolation='none', cmap='viridis')
plt.yticks(range(30), cancer.feature_names)
plt.xlabel("Columns in weight matrix")
plt.ylabel("Input feature")
plt.colorbar()
plt.show()

### uncertainty estimates from classification

from sklearn.datasets import make_blobs, make_circles
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier

X, y = make_circles(noise=0.25, factor=0.5, random_state=1)
print("unique classes: {}".format(np.unique(y)))
y_named = np.array(["blue", "red"])[y]

X_train, X_test, y_train_named, y_test_named, y_train, y_test = train_test_split(X, y_named, y, random_state=0)

## build the model

gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)
gbrt.fit(X_train, y_train_named)

# Changed 'descision_function' to 'decision_function'
gbrt.score(X_test, y_test_named), gbrt.score(X_train, y_train_named)

gbrt.decision_function(X_test)

print("Threshold decision function:\n {}".format(gbrt.decision_function(X_test) > 0))
print("Predictions:\n {}".format(gbrt.predict(X_test)))

y_test

fig, axes = plt.subplots(1, 2, figsize=(13, 5))
mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)
scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1], alpha=.4, cm=mglearn.ReBl)

for ax in axes:
  mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test, ax=ax)
  mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)
  ax.set_xlabel("feature 0")
  ax.set_ylabel("feature 1")

cbar = plt.colorbar(scores_image, ax=axes.tolist())
axes[0].legend(loc="upper left")
axes[1].legend(loc="upper left")
plt.show()

### predicting probability

print("Shape of probabilities: {}".format(gbrt.predict_proba(X_test).shape))

gbrt.predict_proba(X_test)

fig, axes = plt.subplots(1, 2, figsize=(13, 5))
mglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)
scores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1], alpha=.4, cm=mglearn.ReBl, function='predict_proba')

for ax in axes:
  mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test, ax=ax)
  mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)
  ax.set_xlabel("feature 0")
  ax.set_ylabel("feature 1")

cbar = plt.colorbar(scores_image, ax=axes.tolist())
axes[0].legend(loc="upper left")
axes[1].legend(loc="upper left")

### uncertainty in the multiclass classification
## decision_function and predict proba to the iris data set

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

iris = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)

gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)
gbrt.fit(X_train, y_train)

print("Decision function shape: {}".format(gbrt.decision_function(X_test).shape))
print("Argmax of decision function:\n {}".format(gbrt.decision_function(X_test)))

print("argmax of decision function:\n{}".format(np.argmax(gbrt.decision_function(X_test), axis=1)))
print("Predictions:\n{}".format(gbrt.predict(X_test)))

print("Predicted probabilities :\n{}".format(gbrt.predict_proba(X_test)))
print("Sums:{}".format(gbrt.predict_proba(X_test).sum(axis=1)))

print("argmax of predicted probabilities:\n{}".format(np.argmax(gbrt.predict_proba(X_test), axis=1)))
print("Predictions:\n{}".format(gbrt.predict(X_test)))

gbrt.predict(X_train)

